{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af539e3-0148-4bdd-bfd8-fc2c3ec82b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Ai Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e31dc47-49a8-4be1-9299-9772470785da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:23:12.911029Z",
     "iopub.status.busy": "2025-02-11T04:23:12.910118Z",
     "iopub.status.idle": "2025-02-11T04:23:14.913328Z",
     "shell.execute_reply": "2025-02-11T04:23:14.911236Z",
     "shell.execute_reply.started": "2025-02-11T04:23:12.910964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Loading devices dictionary\n",
    "from deviceMatching.getDict import * \n",
    "from deviceMatching.proccessing import load_sentence_transformer, encode_text\n",
    "device_dictionary = load_devices_dictionary_easy()\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4198491f-4ca7-45e5-ab34-909e8429df61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:21:59.779716Z",
     "iopub.status.busy": "2025-02-11T04:21:59.778717Z",
     "iopub.status.idle": "2025-02-11T04:22:05.201148Z",
     "shell.execute_reply": "2025-02-11T04:22:05.199348Z",
     "shell.execute_reply.started": "2025-02-11T04:21:59.779626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (384,)\n",
      "Embedding vector: [-6.37365580e-02  1.53851174e-02 -2.25298647e-02 -4.91679162e-02\n",
      "  7.62720332e-02  2.67352583e-03 -8.31667893e-03  1.00508314e-02\n",
      "  5.59103824e-02 -5.46157099e-02  4.02013734e-02  4.17462550e-02\n",
      "  1.68936141e-02  6.40096366e-02 -3.56010832e-02  3.08003020e-03\n",
      "  2.26955116e-02  1.99567620e-02  8.37309733e-02  3.54791433e-02\n",
      "  7.04875216e-02  6.60389289e-02  7.95279536e-03 -4.05321755e-02\n",
      "  4.82406802e-02  1.60532026e-03 -5.28415777e-02  2.73375325e-02\n",
      " -7.22867064e-03 -3.67305614e-02  1.58938300e-02 -1.32654272e-02\n",
      " -1.15982601e-02 -5.16949147e-02  1.06573857e-01  6.96046576e-02\n",
      "  2.90551018e-02 -1.05756298e-01 -1.89150833e-02  7.37699643e-02\n",
      "  4.12815027e-02  4.00578007e-02  5.82456440e-02 -2.13031583e-02\n",
      " -5.08161820e-03 -5.19382814e-03  7.85274357e-02 -7.98774809e-02\n",
      "  1.60789732e-02  1.38386255e-02 -7.89972842e-02 -2.31923889e-02\n",
      "  2.62691304e-02 -8.20356458e-02  1.24809304e-02 -2.83956379e-02\n",
      "  2.60540517e-03  7.77313020e-03  3.89551148e-02 -9.99304000e-03\n",
      "  1.08905718e-01 -7.03781098e-02  2.22738646e-02 -8.07062387e-02\n",
      " -3.50554450e-03 -1.82483215e-02  2.24007890e-02 -6.43552840e-02\n",
      " -2.75516305e-02 -3.37344781e-02 -3.64285558e-02  4.59636673e-02\n",
      "  1.04570724e-02  4.97202463e-02 -4.88255024e-02  7.55316541e-02\n",
      "  1.00140639e-01  4.33777198e-02  9.00754239e-03 -8.55196938e-02\n",
      " -4.16864753e-02  7.61920167e-03  6.14238679e-02  1.76983438e-02\n",
      "  2.51103323e-02  1.46301482e-02 -2.62466893e-02 -4.42080721e-02\n",
      "  1.55015141e-02  1.16129909e-02  8.13296530e-03 -5.00862710e-02\n",
      " -5.97200962e-03  6.68854546e-03  1.83392595e-03 -6.59508677e-03\n",
      "  1.23098223e-02  1.06716603e-02 -1.96752157e-02  8.27705674e-03\n",
      "  1.16696665e-02  8.43948685e-03  3.20063941e-02  3.39489318e-02\n",
      "  7.95347430e-03  2.91740168e-02 -4.78068665e-02  9.66660827e-02\n",
      " -3.49763706e-02 -5.52923530e-02 -9.35903285e-03  8.74763168e-03\n",
      " -4.18035761e-02  8.82776156e-02  2.54219063e-02 -4.84428182e-02\n",
      "  1.60262845e-02  6.47860486e-03 -8.98665115e-02  2.10805777e-02\n",
      "  5.81941642e-02 -6.77064285e-02  1.61770489e-02 -1.84772594e-03\n",
      " -9.11055226e-03 -8.90884623e-02  9.65280831e-03 -3.09565996e-33\n",
      "  7.48603046e-02 -2.17334889e-02  2.83840075e-02  2.18339860e-02\n",
      "  4.98604998e-02 -9.64632109e-02 -5.78806270e-03 -4.29335646e-02\n",
      " -2.11206134e-02 -3.79471816e-02 -3.81273180e-02  2.85889842e-02\n",
      " -1.91943981e-02  2.02975539e-03 -2.79407427e-02 -2.87202671e-02\n",
      "  3.43212998e-03  5.26274107e-02  3.20201404e-02 -1.83365941e-02\n",
      "  5.08618467e-02  1.52355176e-03  2.70026904e-02 -8.69235173e-02\n",
      "  3.82617079e-02  4.25885385e-03 -4.23257425e-02 -1.63859762e-02\n",
      "  4.00278196e-02 -3.77266854e-02  6.38088286e-02  3.85094285e-02\n",
      "  4.83612679e-02 -2.50197053e-02  4.57239524e-02 -9.89356637e-02\n",
      "  1.20500982e-01  8.76664929e-03 -1.13589309e-01  5.43902814e-02\n",
      "  2.12835614e-02  7.06755882e-03  1.13975117e-03 -5.69273755e-02\n",
      "  5.86124398e-02  2.96904333e-02  2.38802414e-02  1.30799459e-03\n",
      "  4.06641513e-02 -6.24539629e-02 -7.07546398e-02 -4.20588106e-02\n",
      "  1.40135765e-01 -2.10537426e-02  1.14135817e-02  5.72484732e-02\n",
      " -9.51835737e-02 -2.91171148e-02  2.52976939e-02 -6.20691665e-02\n",
      " -1.24362409e-02 -8.71506706e-03  3.57753970e-02 -2.89113522e-02\n",
      "  1.80502832e-02 -9.29939896e-02 -4.07221504e-02  2.50337757e-02\n",
      "  3.51203084e-02  2.85170265e-02  6.12214021e-02 -1.29303873e-01\n",
      "  7.92545602e-02  1.00094147e-01 -1.72443166e-02 -2.62946784e-02\n",
      "  2.27106884e-02 -1.33915907e-02 -9.32911038e-02  2.72912830e-02\n",
      " -2.86356360e-02 -6.80266693e-02  1.19402586e-03 -3.40555869e-02\n",
      " -3.02815605e-02  2.56938580e-02  3.82441096e-02  1.15654711e-02\n",
      "  5.29136695e-02  2.46159639e-02 -6.56760037e-02  1.09614516e-02\n",
      " -7.76027292e-02 -5.07915951e-03 -7.01998621e-02  4.07260349e-34\n",
      "  9.35012698e-02 -4.95192036e-03  6.70742020e-02 -5.21555357e-02\n",
      "  5.94916046e-02 -2.70164069e-02  7.13460445e-02  3.55133563e-02\n",
      " -2.72816047e-02  3.72133702e-02  1.80264190e-03  3.20584215e-02\n",
      "  5.39583564e-02  3.47955227e-02 -6.63213804e-02  1.26143182e-02\n",
      "  5.92985889e-03 -4.69789058e-02  2.24301107e-02  2.42767558e-02\n",
      "  1.75516624e-02  6.34431168e-02 -4.94148135e-02 -1.59258470e-02\n",
      " -2.10214239e-02  4.19514403e-02  2.17618067e-02  2.92622820e-02\n",
      " -1.74761683e-01 -7.09300116e-03  6.76111057e-02 -3.02775786e-03\n",
      "  7.22907018e-03 -1.72619466e-02  7.87021220e-03  4.74859141e-02\n",
      " -2.59298384e-02 -3.73144113e-02 -7.55064040e-02 -9.66686904e-02\n",
      "  5.07764369e-02 -1.15807541e-01  6.34989440e-02 -2.00611148e-02\n",
      " -4.90665436e-03 -7.73825124e-02  6.84084445e-02 -1.12073444e-01\n",
      " -2.69617476e-02 -4.58608447e-05  7.51660839e-02 -3.80197004e-03\n",
      "  4.33379300e-02  9.62794060e-04  2.97745690e-02 -2.48345342e-02\n",
      " -4.38263118e-02  2.99327839e-02  1.68426391e-02  1.23761920e-02\n",
      "  8.78233404e-04  3.97627503e-02 -1.27389297e-01 -2.61580348e-02\n",
      "  6.56112060e-02 -2.73492262e-02 -2.36680917e-02  3.01563907e-02\n",
      " -6.28451407e-02 -6.51880279e-02 -1.04525588e-01  1.71420854e-02\n",
      "  1.10263772e-01  9.41004008e-02 -8.28863028e-03 -5.49283102e-02\n",
      " -4.12328951e-02 -5.49032129e-02 -1.08361440e-02  6.81659505e-02\n",
      " -6.57155067e-02 -9.49514881e-02 -9.96334329e-02 -9.94476676e-03\n",
      " -1.87532837e-03  1.36231361e-02  5.50707020e-02  7.67755806e-02\n",
      "  1.55627960e-03 -2.05205176e-02 -4.03055623e-02 -4.67734970e-02\n",
      "  1.16652763e-03  6.62533119e-02  2.85466872e-02 -1.94134131e-08\n",
      "  3.20429094e-02  9.10377651e-02  8.72168038e-03  4.50575650e-02\n",
      "  8.98803845e-02 -5.04108891e-02  1.11903735e-01 -1.42870545e-02\n",
      "  1.91181470e-02  3.20131369e-02 -4.18746583e-02 -1.27378842e-02\n",
      "  3.02076992e-02  7.18316436e-02 -5.36451042e-02 -8.41614883e-03\n",
      " -9.41710398e-02  7.97648504e-02  3.79471071e-02 -6.57756403e-02\n",
      " -9.52535309e-03 -2.38920413e-02 -4.02778052e-02 -1.89989675e-02\n",
      "  8.12912285e-02  1.05425399e-02  6.11499064e-02 -7.54364282e-02\n",
      " -2.35223230e-02  2.60660090e-02 -5.97025752e-02 -2.49969326e-02\n",
      "  1.92821845e-02 -3.58151011e-02 -1.03856683e-01  3.09467521e-02\n",
      "  7.14719072e-02  5.09658046e-02  3.02659757e-02 -8.96033049e-02\n",
      "  1.23865055e-02  8.98943394e-02  5.81828728e-02 -4.65097502e-02\n",
      " -3.53011340e-02  5.17266430e-02 -1.27507210e-01  8.70989412e-02\n",
      " -7.06730112e-02 -5.64055555e-02  1.63239241e-03 -4.69931997e-02\n",
      " -3.80496122e-03  2.17061769e-02  6.29222617e-02 -2.05708537e-02\n",
      " -1.92294940e-02  1.98815241e-02 -2.14216579e-03  8.96893628e-03\n",
      "  9.49029475e-02 -8.90807062e-02 -2.61891223e-02 -5.75551428e-02]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = load_sentence_transformer(model_name)\n",
    "\n",
    "# Sample device log text\n",
    "device_log = \"Error 404: Resource not found during boot sequence.\"\n",
    "\n",
    "# Encode the text to get a sentence embedding\n",
    "embedding = encode_text(device_log, model)\n",
    "\n",
    "print(\"Embedding shape:\", embedding.shape)  # Should be something like (384,)\n",
    "print(\"Embedding vector:\", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "155fb006-6cac-4019-b156-4a077d2cd36c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:23:21.598162Z",
     "iopub.status.busy": "2025-02-11T04:23:21.596999Z",
     "iopub.status.idle": "2025-02-11T04:23:21.683278Z",
     "shell.execute_reply": "2025-02-11T04:23:21.681641Z",
     "shell.execute_reply.started": "2025-02-11T04:23:21.598088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9118934869766235\n"
     ]
    }
   ],
   "source": [
    "# Define two example strings (e.g., two device logs)\n",
    "string1 = \"Device failed to boot due to error code 404.\"\n",
    "string2 = \"Error 404 encountered during startup; device did not boot.\"\n",
    "\n",
    "# Encode the strings into embeddings (as PyTorch tensors)\n",
    "embedding1 = model.encode(string1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(string2, convert_to_tensor=True)\n",
    "\n",
    "# Compute the cosine similarity between the two embeddings\n",
    "cosine_similarity = util.cos_sim(embedding1, embedding2)\n",
    "\n",
    "# Print the similarity score\n",
    "print(\"Cosine Similarity:\", cosine_similarity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0651424a-4fb1-42f8-a7a5-f20f52b914fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:39:42.085267Z",
     "iopub.status.busy": "2025-02-11T04:39:42.084536Z",
     "iopub.status.idle": "2025-02-11T04:39:42.152398Z",
     "shell.execute_reply": "2025-02-11T04:39:42.148762Z",
     "shell.execute_reply.started": "2025-02-11T04:39:42.085204Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprecompute_corpus_embeddings\u001b[39m(corpus: \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m, model: SentenceTransformer, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, multi_process: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Precompute embeddings for a list of texts (corpus) using the provided SentenceTransformer model.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m        torch.Tensor: A tensor of shape (num_texts, embedding_dim) containing the embeddings.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_process:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# Start a pool of processes (by default, uses all available GPUs/CPUs as specified)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def precompute_corpus_embeddings(corpus: list[str], model: SentenceTransformer, batch_size: int = 32, multi_process: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Precompute embeddings for a list of texts (corpus) using the provided SentenceTransformer model.\n",
    "    \n",
    "    Parameters:\n",
    "        corpus (list[str]): List of input texts.\n",
    "        model (SentenceTransformer): A loaded SentenceTransformer model.\n",
    "        batch_size (int): Batch size for encoding (default: 32).\n",
    "        multi_process (bool): If True, use multi-process encoding; otherwise, use the regular encode().\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (num_texts, embedding_dim) containing the embeddings.\n",
    "    \"\"\"\n",
    "    if multi_process:\n",
    "        # Start a pool of processes (by default, uses all available GPUs/CPUs as specified)\n",
    "        pool = model.start_multi_process_pool()\n",
    "        # Use the multi-process encoding function.\n",
    "        # Note: encode_multi_process returns a NumPy array if convert_to_numpy is True,\n",
    "        # so we set convert_to_numpy=False to get a tensor.\n",
    "        corpus_embeddings = model.encode_multi_process(corpus, pool, batch_size=batch_size, convert_to_numpy=False)\n",
    "        model.stop_multi_process_pool(pool)\n",
    "        # Ensure the result is a torch.Tensor\n",
    "        corpus_embeddings = torch.tensor(corpus_embeddings)\n",
    "    else:\n",
    "        # Regular encoding: the built-in encode() function is optimized with batching and GPU acceleration.\n",
    "        corpus_embeddings = model.encode(corpus, batch_size=batch_size, convert_to_tensor=True)\n",
    "    \n",
    "    return corpus_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be96bc0-628d-413e-bd7e-66e2b8e96745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:52:53.668297Z",
     "iopub.status.busy": "2025-02-11T04:52:53.667478Z",
     "iopub.status.idle": "2025-02-11T04:52:53.703902Z",
     "shell.execute_reply": "2025-02-11T04:52:53.701500Z",
     "shell.execute_reply.started": "2025-02-11T04:52:53.668230Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def precompute_corpus_embeddings(corpus: List[str],\n",
    "                                 model: SentenceTransformer,\n",
    "                                 batch_size: int = 32,\n",
    "                                 multi_process: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Precompute embeddings for a list of texts (corpus) using the provided SentenceTransformer model.\n",
    "    \n",
    "    This function is designed to efficiently process large datasets.\n",
    "    - If multi_process is True, it splits the corpus into chunks and processes them in parallel using multiple processes.\n",
    "    - Otherwise, it uses the built-in encode() method with batching.\n",
    "    \n",
    "    Parameters:\n",
    "        corpus (List[str]): The list of texts to encode.\n",
    "        model (SentenceTransformer): A loaded SentenceTransformer model.\n",
    "        batch_size (int): The number of texts to process per batch.\n",
    "        multi_process (bool): Whether to use multi-process encoding (useful for very large datasets).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (num_texts, embedding_dim) containing the embeddings.\n",
    "    \"\"\"\n",
    "    if multi_process:\n",
    "        # For very large datasets, you can leverage multiple processes.\n",
    "        pool = model.start_multi_process_pool()\n",
    "        # encode_multi_process processes chunks in parallel.\n",
    "        corpus_embeddings = model.encode_multi_process(corpus, pool,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       convert_to_numpy=False)\n",
    "        model.stop_multi_process_pool(pool)\n",
    "        # Convert to torch.Tensor if not already\n",
    "        corpus_embeddings = torch.tensor(corpus_embeddings)\n",
    "    else:\n",
    "        # Regular encode() uses efficient batching and GPU acceleration if available.\n",
    "        corpus_embeddings = model.encode(corpus, batch_size=batch_size, convert_to_tensor=True)\n",
    "    \n",
    "    return corpus_embeddings\n",
    "\n",
    "\n",
    "def normalize_embeddings(embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize each embedding vector to have unit length (L2 normalization).\n",
    "    \n",
    "    Normalization is important because when embeddings are normalized,\n",
    "    the cosine similarity between them is equivalent to their dot product.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings (torch.Tensor): A tensor of shape (N, embedding_dim).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of the same shape with each row normalized.\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "def match_query_to_corpus(query: str,\n",
    "                          corpus: List[str],\n",
    "                          model: SentenceTransformer,\n",
    "                          corpus_embeddings: torch.Tensor) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Encode a query string and find the most similar entry in the precomputed corpus.\n",
    "    \n",
    "    The function encodes the query using the model, normalizes the query embedding (to match the normalized corpus embeddings),\n",
    "    computes cosine similarity with all corpus embeddings, and returns the best matching corpus text along with its score.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The query text (e.g., a device log).\n",
    "        corpus (List[str]): The list of corpus texts (used for returning the actual text).\n",
    "        model (SentenceTransformer): The model used for encoding.\n",
    "        corpus_embeddings (torch.Tensor): Precomputed (and normalized) embeddings for the corpus.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[str, float]: The best matching text from the corpus and its cosine similarity score.\n",
    "    \"\"\"\n",
    "    # Encode the query text.\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Normalize the query embedding for a consistent cosine similarity computation.\n",
    "    query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=0)\n",
    "    \n",
    "    # Compute cosine similarity between the query and all corpus embeddings.\n",
    "    cosine_scores = util.cos_sim(query_embedding, corpus_embeddings)\n",
    "    \n",
    "    # Find the index with the highest similarity.\n",
    "    best_idx = int(torch.argmax(cosine_scores))\n",
    "    best_score = cosine_scores[0][best_idx].item()\n",
    "    \n",
    "    return corpus[best_idx], best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c821a4-fd42-4ed0-ac6e-5d2c76c0030e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:52:36.552888Z",
     "iopub.status.busy": "2025-02-11T04:52:36.550490Z",
     "iopub.status.idle": "2025-02-11T04:52:40.198216Z",
     "shell.execute_reply": "2025-02-11T04:52:40.196509Z",
     "shell.execute_reply.started": "2025-02-11T04:52:36.552788Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Example corpus: this can be as large as 10k+ entries.\n",
    "corpus = [\n",
    "    \"Device failed to boot due to error code 404.\",\n",
    "    \"System booted normally with no issues.\",\n",
    "    \"Warning: Low disk space detected on device.\",\n",
    "    \"Error 404 encountered during startup; device did not boot.\"\n",
    "]\n",
    "\n",
    "corpus_embeddings = precompute_corpus_embeddings(corpus, model, batch_size=32, multi_process=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25e386e2-8b11-43fa-a44f-e898d1c0ee7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:52:56.500772Z",
     "iopub.status.busy": "2025-02-11T04:52:56.500014Z",
     "iopub.status.idle": "2025-02-11T04:52:56.510069Z",
     "shell.execute_reply": "2025-02-11T04:52:56.507814Z",
     "shell.execute_reply.started": "2025-02-11T04:52:56.500710Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_corpus_embeddings = normalize_embeddings(corpus_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05cca526-40d4-482c-bc9e-964e8b924be2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:53:13.746426Z",
     "iopub.status.busy": "2025-02-11T04:53:13.745302Z",
     "iopub.status.idle": "2025-02-11T04:53:13.789895Z",
     "shell.execute_reply": "2025-02-11T04:53:13.788208Z",
     "shell.execute_reply.started": "2025-02-11T04:53:13.746367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching corpus entry: Error 404 encountered during startup; device did not boot.\n",
      "Cosine similarity score: 0.7473016381263733\n"
     ]
    }
   ],
   "source": [
    "# New query text (device log) to match against the corpus\n",
    "query = \"Error 404: Resource not found during boot sequence.\"\n",
    "\n",
    "# Find the best matching corpus entry for the query.\n",
    "best_match, score = match_query_to_corpus(query, corpus, model, norm_corpus_embeddings)\n",
    "\n",
    "print(\"Best matching corpus entry:\", best_match)\n",
    "print(\"Cosine similarity score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb6feb-b4b2-4f85-bad2-215779275814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (matching)",
   "language": "python",
   "name": "matching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
